The goal currently is to parse jQuery in reasonable time and memory.

jQuery 1.3.2 is 120762 characters.

So we will plan in 124000 characters in our estimates.

Currently about 1GiB of memory is used when attempting to parse this, and the parse does not succeed (or I have never waited long enough).

Allocating 124k empty objects in an array (as we do for the state table) allocates about 4MiB in spidermonkey, so that's not the problem.

js> gc()
before 184320, after 61440, break 080f1000
js> for(var i=0;i<124000;i++)a.push({})
124000
js> gc()
before 4177920, after 4177920, break 08899000

We next create a 124000 character string, and then attempt to populate the table with tails of the string:

js> s=Array(124001).join('.')
............................... [...]
js> for(var i=0;i<124000;i++)a[i].str=s.slice(i)

Soon after we kill the process...

I had hoped that slice() would create a reference to an offset into an existing string but apparently this is not how it is implemented, and an actual copy is made.
Well, perhaps that actually is how it's done:

js> a=[]

js> for(i=0;i<4096;i++)a[i]={}
[object Object]
js> a.length
4096
js> s=Array(4097).join('.');s.length
4096
js> gc()
before 155648, after 155648, break 080d8000
js> for(i=0;i<4096;i++)a[i].str=s.slice(i)
.
js> gc()
before 196608, after 196608, break 080f9000
js> delete a; delete s
true
js> gc()
before 196608, after 28672, break 080f9000
js> a=[];for(i=0;i<16384;i++)a.push({})
16384
js> s=Array(16385).join('.');s.length
16384
js> gc()
before 569344, after 569344, break 081da000
js> for(i=0;i<16384;i++)a[i].str=s.slice(i)
.
js> gc()
before 716800, after 716800, break 0825e000
js> 

This is bizarre and seems incredibly non-linear

js> gc()
before 20480, after 20480, break 08094000
js> s=Array(32769).join('.');s.length
32768
js> gc()
before 20480, after 20480, break 08094000
js> a=[];for(i=0;i<32768;i++)a[i]={str:s.slice(i)}
[object Object]
js> gc()
before 1409024, after 1409024, break 39b97000
js> countHeap()
65902
js> // the OS reports that the process has ballooned to 801MB
js> delete s
true
js> gc()
before 1409024, after 1409024, break 39bb7000
js> delete a
true
js> gc()
before 1409024, after 24576, break 32ab0000
js> countHeap()
362
js> 

Fixing this so that the slices are not saved reduced the memory usage even on moderate (16k) inputs by a huge amount, from a Firefox process size of around 600MiB to staying around 200MiB (which is where it started).
We can now actually parse jQuery.
Memory usage is reasonable.
jQuery v1.3.2, minified (57254 characters) parses in 192.798s.
The un-minified version of the same file (120762 characters) parses in 321.656s.

This is unacceptably slow, but it's also much better than it was.
Next I will add some profiling information to the generated code.

20090906

jQuery v1.3.2 does not parse at all using the ES5 parser, as it contains a function getWH() which is inside a block in an if statement, which is not permitted by the grammar.

If this function is moved outside of the if statement, the entire file successfully parses in 29.6 seconds.
This is on the same hardware as the earlier 321.7s result, so we have a speedup of about 10 times, consistent with the benchmarking done on much smaller inputs and simpler grammars.

Some further optimization of what gets added to the result table brings our time to parse jQuery down to 17.1s.
However it should be noted that the current parser is no longer constructing a parse tree, this is a separate step which needs to be added in again.
Parsing jQuery and building the tree now takes about 20s.

Using integer indices (of the rule's position in the given PEG) rather than rule names, and arrays rather than objects, in the result table, reduces the runtime to about 17s for the same task of parsing and building a tree for jQuery (but not returning the result from the worker).
Of this, building the result tree, which is a separate step at the end, takes only an additional second or so.